{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Tv3jKe4syG"
   },
   "outputs": [],
   "source": [
    "# Checklist:\n",
    "# AWS emr-5.29.0\n",
    "# MASTER r5d.8xlarge 1x, no EBS\n",
    "# CORE r5d.8xlarge 4x, no EBS\n",
    "# Custom bootstrap action: s3://ydatazian/bootstrap.sh\n",
    "# Allow ssh in master node security group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "CGHGHkqE4rJw",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3duPlU1fjxPz"
   },
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "u566smRWkDOS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameNode: http://ec2-3-218-152-40.compute-1.amazonaws.com:50070\n",
      "YARN: http://ec2-3-218-152-40.compute-1.amazonaws.com:8088\n",
      "Spark UI: http://ec2-3-218-152-40.compute-1.amazonaws.com:20888/proxy/application_1620976236162_0003\n"
     ]
    }
   ],
   "source": [
    "# connect, context, session\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import spark_utils\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"yarn\", \"My App\", conf=spark_utils.get_spark_conf())\n",
    "\n",
    "spark_utils.print_ui_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "BxHvUF7qpEHO"
   },
   "outputs": [],
   "source": [
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCrxvDnx47f2"
   },
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-tZacCcy49Lv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                   Size     Used  Available  Use%\r\n",
      "hdfs://ip-172-31-7-108.ec2.internal:8020  547.5 G  222.3 M    546.0 G    0%\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1m8mBXzZ4-kB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxrwxrwt   - hdfs hadoop          0 2021-05-14 07:10 /tmp\r\n",
      "drwxr-xr-x   - hdfs hadoop          0 2021-05-14 07:10 /user\r\n",
      "drwxr-xr-x   - hdfs hadoop          0 2021-05-14 07:10 /var\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrcHwAaQjxam"
   },
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bakU6PKuj-0s"
   },
   "source": [
    "RDD (Resilient Distributed Datasets) - base data block of Spark. The system takes care about parts of the data and it's manipulations on distributed system. It could be treated as ordered sequence of rows (commonly key-value pairs like in MapReduce, but could be any arbitrary data).\n",
    "\n",
    "RDDs are immutable. You get new RDD by making operations on initial RDD.\n",
    "\n",
    "There is two kinds of operations on RDD: *actions* and *transformations*.\n",
    "\n",
    "Transformations are not applied instantly, they are stacked in operations order.\n",
    "\n",
    "Actions are used to materialize transformations (so the data is actually transformed on cluster).\n",
    "\n",
    "Documentation: https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PLjBvt5YmiuU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create simple RDD first\n",
    "rdd = sc.parallelize(range(10))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPC6YOl-lLM3"
   },
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Y2Xhvx9lj8YB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() # gather data into python, be careful, loads data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FoCGZgLimxcA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count() # returns count of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YLWX_3kymxfU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first() # get first element of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KqYXt8cdrMC5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2) # get first N=2 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IcDLdYsSrPs3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mean() # mean of RDD's values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8fhwaR-OrTZI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[12] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can create RDD with text data\n",
    "rdd = sc.parallelize([\"one\", \"two\"])\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_aZFNOvprjNa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'two']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() # get RDD values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QVBjW4qirmGE"
   },
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"/tmp_text.txt\")  # save RDD into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "U9Lim1dJrsYo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 501 items\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/_SUCCESS\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00000\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00001\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00002\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00003\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00004\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00005\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00006\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00007\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00008\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00009\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00010\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00011\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00012\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00013\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00014\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00015\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00016\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00017\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00018\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00019\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00020\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00021\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00022\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00023\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00024\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00025\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00026\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00027\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00028\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00029\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00030\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00031\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00032\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00033\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00034\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00035\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00036\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00037\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00038\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00039\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00040\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00041\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00042\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00043\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00044\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00045\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00046\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00047\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00048\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00049\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00050\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00051\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00052\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00053\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00054\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00055\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00056\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00057\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00058\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00059\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00060\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00061\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00062\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00063\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00064\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00065\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00066\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00067\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00068\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00069\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00070\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00071\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00072\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00073\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00074\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00075\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00076\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00077\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00078\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00079\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00080\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00081\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00082\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00083\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00084\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00085\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00086\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00087\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00088\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00089\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00090\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00091\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00092\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00093\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00094\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00095\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00096\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00097\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00098\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00099\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00100\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00101\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00102\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00103\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00104\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00105\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00106\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00107\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00108\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00109\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00110\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00111\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00112\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00113\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00114\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00115\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00116\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00117\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00118\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00119\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00120\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00121\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00122\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00123\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00124\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00125\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00126\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00127\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00128\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00129\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00130\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00131\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00132\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00133\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00134\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00135\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00136\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00137\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00138\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00139\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00140\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00141\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00142\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00143\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00144\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00145\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00146\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00147\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00148\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00149\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00150\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00151\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00152\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00153\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00154\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00155\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00156\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00157\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00158\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00159\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00160\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00161\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00162\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00163\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00164\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00165\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00166\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00167\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00168\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00169\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00170\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00171\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00172\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00173\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00174\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00175\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00176\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00177\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00178\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00179\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00180\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00181\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00182\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00183\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00184\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00185\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00186\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00187\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00188\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00189\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00190\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00191\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00192\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00193\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00194\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00195\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00196\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00197\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00198\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00199\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00200\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00201\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00202\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00203\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00204\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00205\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00206\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00207\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00208\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00209\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00210\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00211\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00212\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00213\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00214\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00215\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00216\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00217\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00218\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00219\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00220\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00221\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00222\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00223\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00224\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00225\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00226\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00227\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00228\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00229\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00230\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00231\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00232\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00233\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00234\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00235\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00236\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00237\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00238\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00239\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00240\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00241\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00242\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00243\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00244\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00245\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00246\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00247\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00248\n",
      "-rw-r--r--   1 hadoop hadoop          4 2021-05-14 07:19 /tmp_text.txt/part-00249\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00250\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00251\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00252\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00253\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00254\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00255\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00256\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00257\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00258\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00259\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00260\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00261\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00262\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00263\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00264\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00265\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00266\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00267\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00268\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00269\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00270\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00271\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00272\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00273\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00274\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00275\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00276\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00277\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00278\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00279\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00280\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00281\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00282\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00283\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00284\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00285\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00286\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00287\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00288\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00289\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00290\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00291\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00292\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00293\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00294\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00295\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00296\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00297\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00298\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00299\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00300\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00301\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00302\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00303\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00304\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00305\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00306\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00307\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00308\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00309\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00310\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00311\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00312\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00313\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00314\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00315\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00316\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00317\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00318\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00319\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00320\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00321\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00322\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00323\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00324\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00325\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00326\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00327\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00328\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00329\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00330\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00331\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00332\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00333\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00334\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00335\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00336\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00337\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00338\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00339\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00340\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00341\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00342\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00343\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00344\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00345\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00346\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00347\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00348\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00349\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00350\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00351\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00352\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00353\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00354\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00355\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00356\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00357\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00358\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00359\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00360\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00361\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00362\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00363\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00364\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00365\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00366\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00367\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00368\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00369\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00370\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00371\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00372\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00373\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00374\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00375\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00376\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00377\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00378\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00379\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00380\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00381\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00382\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00383\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00384\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00385\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00386\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00387\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00388\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00389\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00390\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00391\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00392\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00393\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00394\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00395\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00396\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00397\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00398\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00399\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00400\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00401\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00402\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00403\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00404\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00405\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00406\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00407\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00408\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00409\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00410\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00411\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00412\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00413\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00414\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00415\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00416\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00417\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00418\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00419\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00420\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00421\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00422\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00423\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00424\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00425\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00426\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00427\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00428\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00429\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00430\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00431\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00432\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00433\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00434\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00435\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00436\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00437\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00438\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00439\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00440\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00441\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00442\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00443\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00444\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00445\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00446\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00447\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00448\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00449\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00450\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00451\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00452\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00453\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00454\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00455\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00456\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00457\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00458\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00459\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00460\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00461\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00462\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00463\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00464\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00465\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00466\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00467\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00468\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00469\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00470\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00471\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00472\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00473\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00474\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00475\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00476\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00477\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00478\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00479\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00480\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00481\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00482\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00483\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00484\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00485\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00486\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00487\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00488\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00489\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00490\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00491\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00492\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00493\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00494\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00495\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00496\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00497\n",
      "-rw-r--r--   1 hadoop hadoop          0 2021-05-14 07:19 /tmp_text.txt/part-00498\n",
      "-rw-r--r--   1 hadoop hadoop          4 2021-05-14 07:19 /tmp_text.txt/part-00499\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -ls /tmp_text.txt # parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eIKRO5D3rzeC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "two\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -cat /tmp_text.txt/* # actual data from parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kk4wsCJ1lOUd"
   },
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iruV4O0hlObo"
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(10)) # RDD from range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2L3-6lOpmln7"
   },
   "outputs": [],
   "source": [
    "# New RDD with \"square + 1\" transformation by two map operations;\n",
    "# Map operations are similar to those from MapReduce,\n",
    "# the difference - given map functions are applied to each element of rdd:\n",
    "squares = rdd.map(lambda x: x**2).map(lambda x: x + 1)\n",
    "\n",
    "# IMPORTANT NOTE - nothing is calculated right now,\n",
    "# `squares` now only represents sequence for new RDD over initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-KcdXAyZmyMG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares.first() \n",
    "\n",
    "# Now we applied Action, so the map transformations are run\n",
    "# But not all data is calculated, Spark optimized that for us\n",
    "# and only the value for first row was calculated and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DyY3JBSmt09P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 5, 10, 17, 26, 37, 50, 65, 82]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares.collect() # get all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IxQRwjxVvrS_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 10, 26, 37, 50, 65, 82]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares.sample(False, 0.5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "j927zl3DwD5q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 2, 3]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares.flatMap(lambda x: [x, x+1, x+2]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1OrvtNaJwG3H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[82]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    squares\n",
    "    .takeOrdered(1, lambda x: -x) # top 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8ivcPWBTfglT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x % 2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7QmOvTlmyWj"
   },
   "source": [
    "### MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tZpXudV_mz12"
   },
   "outputs": [],
   "source": [
    "# step by step MapReduce simulation:\n",
    "rdd = sc.parallelize([\"this is text\", \"some more text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "x862XjL0m0a-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 1), ('is', 1), ('text', 1), ('some', 1), ('more', 1), ('text', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    rdd\n",
    "    .flatMap(lambda x: [(w, 1) for w in x.split()])\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "GhOYxLaR2Qr_"
   },
   "outputs": [],
   "source": [
    "# we gonna use first iteration often, to save time we can cache the result:\n",
    "words = rdd.flatMap(lambda x: [(w, 1) for w in x.split()]).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42qUIO7SgXil"
   },
   "source": [
    "**PairRDD**\n",
    "\n",
    "If you have a tuple of length 2 as your RDD data type, you can use *ByKey operations on your RDD, with first value of tuple being the key and second being the value. Let's create such RDD.\n",
    "\n",
    "We want to aggregate data by key (word), we are able to do it with `groupByKey` method, it will produce values iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "OXWh_qA0m0uH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', <pyspark.resultiterable.ResultIterable at 0x7f9b4ccefba8>),\n",
       " ('more', <pyspark.resultiterable.ResultIterable at 0x7f9b4ccefb70>),\n",
       " ('this', <pyspark.resultiterable.ResultIterable at 0x7f9b4ccefc18>),\n",
       " ('some', <pyspark.resultiterable.ResultIterable at 0x7f9b4ccefc88>),\n",
       " ('is', <pyspark.resultiterable.ResultIterable at 0x7f9b4ccefcf8>)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    words\n",
    "    .groupByKey()\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJeahVF0glWe"
   },
   "source": [
    "And of course we can use any function in map, not just lambdas,\n",
    "\n",
    "with regular `map` function you can change key/value, create complex keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nw5USXbkxvOe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [1, 1], 'more': [1], 'this': [1], 'some': [1], 'is': [1]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapToList(x):\n",
    "  return x[0], list(x[1])\n",
    "\n",
    "data = (\n",
    "    words\n",
    "    .groupByKey()\n",
    "    .map(mapToList)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEtvZG4biDVl"
   },
   "source": [
    "We may use `.mapValues` method to manipulate only with values and leave keys intact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "hnXI092aiKCq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 2), ('more', 1), ('this', 1), ('some', 1), ('is', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapValuesToLen(x):\n",
    "  return len(x)\n",
    "\n",
    "(\n",
    "    words\n",
    "    .groupByKey()\n",
    "    .mapValues(mapValuesToLen)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "g6dQHKHsxvQq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 2), ('more', 1), ('this', 1), ('some', 1), ('is', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    words\n",
    "    .groupByKey()\n",
    "    .map(lambda x: (x[0], len(x[1])))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiRdwghKhw5w"
   },
   "source": [
    "Another way to manipulate values grouped by key is reduce performed by `reduceByKey` operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "zSL9x0vpxvTB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 2), ('more', 1), ('this', 1), ('some', 1), ('is', 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    words # -> (word, cnt=1)\n",
    "    .reduceByKey(lambda a, b: a + b) # -> (word, sum(cnt))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "lK4YatXIy-zx"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 249 in stage 45.0 failed 4 times, most recent failure: Lost task 249.3 in stage 45.0 (TID 11346, ip-172-31-15-106.ec2.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-fda08014ba43>\", line 5, in <lambda>\n  File \"<ipython-input-40-fda08014ba43>\", line 5, in <listcomp>\nTypeError: object of type 'int' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-fda08014ba43>\", line 5, in <lambda>\n  File \"<ipython-input-40-fda08014ba43>\", line 5, in <listcomp>\nTypeError: object of type 'int' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-fda08014ba43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# len from int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# -> (word, sum(cnt))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 249 in stage 45.0 failed 4 times, most recent failure: Lost task 249.3 in stage 45.0 (TID 11346, ip-172-31-15-106.ec2.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-fda08014ba43>\", line 5, in <lambda>\n  File \"<ipython-input-40-fda08014ba43>\", line 5, in <listcomp>\nTypeError: object of type 'int' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 352, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1861, in combineLocally\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1620976236162_0002/container_1620976236162_0002_01_000004/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-fda08014ba43>\", line 5, in <lambda>\n  File \"<ipython-input-40-fda08014ba43>\", line 5, in <listcomp>\nTypeError: object of type 'int' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# how errors look like:\n",
    "\n",
    "(\n",
    "    rdd\n",
    "    .flatMap(lambda x: [(w, len(1)) for w in x.split()]) # len from int\n",
    "    .reduceByKey(lambda a, b: a + b) # -> (word, sum(cnt))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Kkm691F4Dkz"
   },
   "source": [
    "#### It is possible to manipulate amount of map operations and partitions of the data, we will discuss it next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-cLpcEUfzRv"
   },
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "vh_HXjrgfyyX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of', 'course', 'we', 'can', 'use']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = sc.parallelize(['Of course we can use predefined functions with map and not just lambda',\n",
    "                       'Imagine we want to have each element in the RDD as a key-value pair where the key is the tag (e.g. normal) and the value is the whole list of elements that represents the row in the CSV formatted file', \n",
    "                       'We could proceed as follows'])\n",
    "words = texts.flatMap(lambda x: x.lower().split(' '))\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "F9nQ4ju6f6tw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('element', 1),\n",
       " ('key-value', 1),\n",
       " ('in', 2),\n",
       " ('of', 2),\n",
       " ('not', 1),\n",
       " ('and', 2),\n",
       " ('csv', 1),\n",
       " ('with', 1),\n",
       " ('lambda', 1),\n",
       " ('use', 1),\n",
       " ('formatted', 1),\n",
       " ('elements', 1),\n",
       " ('just', 1),\n",
       " ('course', 1),\n",
       " ('tag', 1),\n",
       " ('map', 1),\n",
       " ('imagine', 1),\n",
       " ('follows', 1),\n",
       " ('rdd', 1),\n",
       " ('that', 1),\n",
       " ('file', 1),\n",
       " ('a', 1),\n",
       " ('pair', 1),\n",
       " ('have', 1),\n",
       " ('predefined', 1),\n",
       " ('the', 7),\n",
       " ('represents', 1),\n",
       " ('we', 3),\n",
       " ('whole', 1),\n",
       " ('where', 1),\n",
       " ('to', 1),\n",
       " ('is', 2),\n",
       " ('list', 1),\n",
       " ('as', 2),\n",
       " ('can', 1),\n",
       " ('(e.g.', 1),\n",
       " ('value', 1),\n",
       " ('key', 1),\n",
       " ('proceed', 1),\n",
       " ('each', 1),\n",
       " ('row', 1),\n",
       " ('could', 1),\n",
       " ('normal)', 1),\n",
       " ('want', 1),\n",
       " ('functions', 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q1OO-vOyY6L"
   },
   "source": [
    "## Broadcast and accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "RjypNMTVyYOp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[87] at RDD at PythonRDD.scala:53\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "errors: 1\n"
     ]
    }
   ],
   "source": [
    "bc = sc.broadcast({\"this\": 0, \"is\": 1, \"text\": 2})\n",
    "errors = sc.accumulator(0)\n",
    "\n",
    "def mapper(x):\n",
    "    global errors\n",
    "    for w in x.split():\n",
    "        if w in bc.value:\n",
    "            yield (bc.value[w], 1)\n",
    "        else:\n",
    "            errors += 1\n",
    "\n",
    "rdd = (\n",
    "    sc\n",
    "   .parallelize([\"this is text\", \"text too\"])\n",
    "   .flatMap(mapper)\n",
    "   .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "print(rdd)\n",
    "print(rdd.collect())\n",
    "print(\"errors:\", errors.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHR7Vzn_jxlF"
   },
   "source": [
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDbdsczvj_i8"
   },
   "source": [
    "RDD is much better and useful than plain MapReduce, but Spark can do even more!\n",
    "Spark DataFrame is table structure over RDDs and can be treated as pandas on steroids.\n",
    "\n",
    "It allows us to perform structured queries and benefit from it. One way is to perform SQL-styled queries (will discuss on next lesson) and another is DataFrame API.\n",
    "\n",
    "Documentation: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "UD21BVEqj3zA"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "EpYvw_HymmTQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 2), ('b', 3), ('b', 4)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"a\", 2), (\"b\", 3), (\"b\", 4)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "i4mxAmRemmb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  2|\n",
      "|  b|  3|\n",
      "|  b|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = se.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "FGJOiQS_2G_-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col_one: string (nullable = true)\n",
      " |-- col_two: long (nullable = true)\n",
      "\n",
      "+-------+-------+\n",
      "|col_one|col_two|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      a|      2|\n",
      "|      b|      3|\n",
      "|      b|      4|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = se.createDataFrame(\n",
    "    rdd.map(lambda x: Row(col_one=x[0], col_two=x[1]))\n",
    ")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "u1RymT0N7n8z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|col_one|\n",
      "+-------+\n",
      "|      a|\n",
      "|      a|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['col_one']).limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "wwkvZly3774s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'a']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(['col_one']).distinct().rdd.map(lambda x: x.col_one).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs: https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "D_QIAytR70kc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|col_one|col_two|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      a|      2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "(\n",
    "  df.select(['col_one', 'col_two'])\n",
    "    .where(F.col('col_one') == 'a')\n",
    "    .limit(2)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "rNp94hJnjHqW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|col_one|col_two|col_two_float|\n",
      "+-------+-------+-------------+\n",
      "|      a|      1|          1.0|\n",
      "|      a|      2|          2.0|\n",
      "|      b|      3|          3.0|\n",
      "|      b|      4|          4.0|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select('*', df['col_two'].cast('float').alias('col_two_float'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "fMgcNpPmjTUM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|col_one|col_two_square|\n",
      "+-------+--------------+\n",
      "|      b|          16.0|\n",
      "|      b|           9.0|\n",
      "|      a|           4.0|\n",
      "|      a|           1.0|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "square_df = df.select('col_one', (df['col_two_float'] * df['col_two_float']).alias('col_two_square'))\n",
    "square_df.orderBy('col_two_square', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "6TO3OLMN7W51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|col_one|col_two_list|\n",
      "+-------+------------+\n",
      "|      b|      [3, 4]|\n",
      "|      a|      [1, 2]|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "(\n",
    "    df\n",
    "      .groupby('col_one')\n",
    "      .agg(F.collect_list(\"col_two\").alias(\"col_two_list\"))\n",
    "      .select(['col_one', 'col_two_list'])\n",
    "      .limit(10)\n",
    "      .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "lc31NQ0x2Lv-"
   },
   "source": [
    "# convertable to Pandas\n",
    "pandas_df = df.toPandas()\n",
    "pandas_df\n",
    "\n",
    "# load from Pandas\n",
    "df = se.createDataFrame(pandas_df)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_one='a', col_two=1, col_two_float=1.0),\n",
       " Row(col_one='a', col_two=2, col_two_float=2.0),\n",
       " Row(col_one='b', col_two=3, col_two_float=3.0),\n",
       " Row(col_one='b', col_two=4, col_two_float=4.0)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.toLocalIterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = se.createDataFrame(df.toLocalIterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col_one: string (nullable = true)\n",
      " |-- col_two: long (nullable = true)\n",
      " |-- col_two_float: double (nullable = true)\n",
      "\n",
      "+-------+-------+-------------+\n",
      "|col_one|col_two|col_two_float|\n",
      "+-------+-------+-------------+\n",
      "|      a|      1|          1.0|\n",
      "|      a|      2|          2.0|\n",
      "|      b|      3|          3.0|\n",
      "|      b|      4|          4.0|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "PYlbgkZI3QiH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_one='a', col_two=1, col_two_float=1.0),\n",
       " Row(col_one='a', col_two=2, col_two_float=2.0),\n",
       " Row(col_one='b', col_two=3, col_two_float=3.0),\n",
       " Row(col_one='b', col_two=4, col_two_float=4.0)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also can get RDD from DF\n",
    "df.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi3keNw6mteI"
   },
   "source": [
    "## Data formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "Mt8434r9mvr2"
   },
   "outputs": [],
   "source": [
    "# We may want to operate with not just plain text, but something more complex\n",
    "# For example, Parquet - it can be useful for huge datasets for faster calcs\n",
    "df.write.save(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "zxDJLG6pmvwf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_one='a', col_two=1, col_two_float=1.0),\n",
       " Row(col_one='a', col_two=2, col_two_float=2.0),\n",
       " Row(col_one='b', col_two=3, col_two_float=3.0),\n",
       " Row(col_one='b', col_two=4, col_two_float=4.0)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = se.read.parquet(\"data.parquet\")\n",
    "data.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1NJypwklF_Z"
   },
   "source": [
    "## Outbrain click prediction dataseet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twX4Yeez5Loj"
   },
   "source": [
    "https://www.kaggle.com/c/outbrain-click-prediction/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXkum5Ay5DID"
   },
   "source": [
    "### AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "j4FsSCqe5CvG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE week1/\r\n",
      "2021-05-06 13:10:24       1672 bootstrap.sh\r\n",
      "2021-05-13 21:20:22  176843889 clicks_test.parquet\r\n",
      "2021-05-13 21:20:22  495815517 clicks_train.parquet\r\n",
      "2021-05-13 21:21:58   34267065 documents_categories.parquet\r\n",
      "2021-05-13 21:21:58  206455957 documents_entities.parquet\r\n",
      "2021-05-13 21:21:58   23859965 documents_meta.parquet\r\n",
      "2021-05-13 21:21:58  187410196 documents_topics.parquet\r\n",
      "2021-05-13 21:21:58  734643471 events.parquet\r\n",
      "2021-05-13 21:56:44 50764611872 page_views.parquet\r\n",
      "2021-05-13 21:21:58  248421413 page_views_sample.parquet\r\n",
      "2021-05-13 21:21:59    5116927 promoted_content.parquet\r\n",
      "2021-05-13 21:21:58  273136709 sample_submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls s3://ydatazian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "V5Qx5EkolI_0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "|8205775c5387f9|        120| 44196592|       1|       IN>16|             2|\n",
      "|9cb0ccd8458371|        120| 65817371|       1|   US>CA>807|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "df = se.read.parquet(\"s3://ydatazian/page_views.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3orFyft25Tk_"
   },
   "source": [
    "### Data manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "lUkB7pVjmnAB"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bba629793ac4c109dab527ab661066b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_test\n",
      "+----------+------+\n",
      "|display_id| ad_id|\n",
      "+----------+------+\n",
      "|  16874594| 66758|\n",
      "|  16874594|150083|\n",
      "|  16874594|162754|\n",
      "+----------+------+\n",
      "\n",
      "clicks_train\n",
      "+----------+------+-------+\n",
      "|display_id| ad_id|clicked|\n",
      "+----------+------+-------+\n",
      "|         1| 42337|      0|\n",
      "|         1|139684|      0|\n",
      "|         1|144739|      1|\n",
      "+----------+------+-------+\n",
      "\n",
      "documents_categories\n",
      "+-----------+-----------+----------------+\n",
      "|document_id|category_id|confidence_level|\n",
      "+-----------+-----------+----------------+\n",
      "|    1595802|       1611|            0.92|\n",
      "|    1595802|       1610|            0.07|\n",
      "|    1524246|       1807|            0.92|\n",
      "+-----------+-----------+----------------+\n",
      "\n",
      "documents_entities\n",
      "+-----------+--------------------+-----------------+\n",
      "|document_id|           entity_id| confidence_level|\n",
      "+-----------+--------------------+-----------------+\n",
      "|    1524246|f9eec25663db4cd83...|0.672865314504701|\n",
      "|    1524246|55ebcfbdaff1d6f60...|0.399113728441297|\n",
      "|    1524246|839907a972930b17b...|0.392095749652966|\n",
      "+-----------+--------------------+-----------------+\n",
      "\n",
      "documents_meta\n",
      "+-----------+---------+------------+-------------------+\n",
      "|document_id|source_id|publisher_id|       publish_time|\n",
      "+-----------+---------+------------+-------------------+\n",
      "|    1595802|        1|         603|2016-06-05 00:00:00|\n",
      "|    1524246|        1|         603|2016-05-26 11:00:00|\n",
      "|    1617787|        1|         603|2016-05-27 00:00:00|\n",
      "+-----------+---------+------------+-------------------+\n",
      "\n",
      "documents_topics\n",
      "+-----------+--------+------------------+\n",
      "|document_id|topic_id|  confidence_level|\n",
      "+-----------+--------+------------------+\n",
      "|    1595802|     140|0.0731131601068925|\n",
      "|    1595802|      16|0.0594164867373976|\n",
      "|    1595802|     143|0.0454207537554526|\n",
      "+-----------+--------+------------------+\n",
      "\n",
      "events\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n",
      "|         2|79a85fa78311b9|    1794259|       81|       2|   US>CA>807|\n",
      "|         3|822932ce3d8757|    1179111|      182|       2|   US>MI>505|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "\n",
      "page_views\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "\n",
      "page_views_sample\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "\n",
      "promoted_content\n",
      "+-----+-----------+-----------+-------------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|\n",
      "+-----+-----------+-----------+-------------+\n",
      "|    1|       6614|          1|            7|\n",
      "|    2|     471467|          2|            7|\n",
      "|    3|       7692|          3|            7|\n",
      "+-----+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "tables = [\"clicks_test\", \"clicks_train\", \n",
    "          \"documents_categories\", \"documents_entities\", \"documents_meta\", \"documents_topics\", \n",
    "          \"events\", \"page_views\", \"page_views_sample\", \"promoted_content\"]\n",
    "for name in tqdm.tqdm(tables):\n",
    "    df = se.read.parquet(\"s3://ydatazian/{}.parquet\".format(name))\n",
    "    df.registerTempTable(name)\n",
    "    print(name)\n",
    "    df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "dyN3IF_LmnCZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[uuid: string, document_id: string, timestamp: string, platform: string, geo_location: string, traffic_source: string]\n"
     ]
    }
   ],
   "source": [
    "page_views = se.table(\"page_views\")\n",
    "print(page_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "|8205775c5387f9|        120| 44196592|       1|       IN>16|             2|\n",
      "|9cb0ccd8458371|        120| 65817371|       1|   US>CA>807|             2|\n",
      "|2aa611f32875c7|        120| 71495491|       1|       CA>ON|             2|\n",
      "|f55a6eaf2b34ab|        120| 73309199|       1|       BR>27|             2|\n",
      "|cc01b582c8cbff|        120| 50033577|       1|       CA>BC|             2|\n",
      "|6c802978b8dd4d|        120| 66590306|       1|       CA>ON|             2|\n",
      "|f4e423314303ff|        120| 48314254|       1|   US>LA>622|             1|\n",
      "|3937372ca2709b|        120| 24360074|       1|          NO|             2|\n",
      "|31f8d101c6a851|        120| 13847456|       1|       PH>D9|             2|\n",
      "|67606983fe1acf|        984| 82129416|       2|   US>OK>671|             2|\n",
      "|3f9d5b09ac4a0a|        984| 82583638|       2|          US|             2|\n",
      "|3dd8a359aa8699|        984| 59439284|       2|   US>MD>511|             2|\n",
      "|95e966c81b9316|        984|   983791|       3|   US>AL>698|             2|\n",
      "|483cf1feb3e47c|        984| 17944009|       2|   US>MI>505|             1|\n",
      "|f7baf0caf201ca|        984| 37096677|       2|       CA>AB|             2|\n",
      "|7a408fe90c02b3|        984| 30457643|       1|          PH|             1|\n",
      "|9ee2877617838a|        984| 86093121|       3|   US>MN>613|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_views.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "|8205775c5387f9|        120| 44196592|       1|       IN>16|             2|\n",
      "|9cb0ccd8458371|        120| 65817371|       1|   US>CA>807|             2|\n",
      "|2aa611f32875c7|        120| 71495491|       1|       CA>ON|             2|\n",
      "|f55a6eaf2b34ab|        120| 73309199|       1|       BR>27|             2|\n",
      "|cc01b582c8cbff|        120| 50033577|       1|       CA>BC|             2|\n",
      "|6c802978b8dd4d|        120| 66590306|       1|       CA>ON|             2|\n",
      "|f4e423314303ff|        120| 48314254|       1|   US>LA>622|             1|\n",
      "|3937372ca2709b|        120| 24360074|       1|          NO|             2|\n",
      "|31f8d101c6a851|        120| 13847456|       1|       PH>D9|             2|\n",
      "|67606983fe1acf|        984| 82129416|       2|   US>OK>671|             2|\n",
      "|3f9d5b09ac4a0a|        984| 82583638|       2|          US|             2|\n",
      "|3dd8a359aa8699|        984| 59439284|       2|   US>MD>511|             2|\n",
      "|95e966c81b9316|        984|   983791|       3|   US>AL>698|             2|\n",
      "|483cf1feb3e47c|        984| 17944009|       2|   US>MI>505|             1|\n",
      "|f7baf0caf201ca|        984| 37096677|       2|       CA>AB|             2|\n",
      "|7a408fe90c02b3|        984| 30457643|       1|          PH|             1|\n",
      "|9ee2877617838a|        984| 86093121|       3|   US>MN>613|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_views_sql = se.sql(\"SELECT * from page_views\")\n",
    "page_views_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "3DWOaQBSmnJT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[document_id: string, topic_id: string, confidence_level: string]\n"
     ]
    }
   ],
   "source": [
    "documents_topics = se.table(\"documents_topics\")\n",
    "print(documents_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxLQczih7AhS"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "(\n",
    "    page_views\n",
    "      .join(documents_topics, page_views.document_id == documents_topics.document_id, 'outer')\n",
    "      .select(page_views.document_id, documents_topics.topic_id)\n",
    "      .sort(desc(\"document_id\"))\n",
    "      .limit(50)\n",
    "      .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|document_id|              topics|\n",
      "+-----------+--------------------+\n",
      "|     100010|[16, 254, 192, 25...|\n",
      "|    1000240|[138, 143, 89, 20...|\n",
      "|    1000280|[183, 199, 235, 279]|\n",
      "|    1000665|           [183, 35]|\n",
      "|    1000795|           [183, 35]|\n",
      "|    1000839|[184, 183, 235, 1...|\n",
      "|    1000888|       [75, 143, 64]|\n",
      "|     100140|      [112, 24, 181]|\n",
      "|    1001866|[269, 97, 214, 43...|\n",
      "|    1002011|[137, 150, 119, 1...|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "  documents_topics\n",
    "    .groupby('document_id')\n",
    "    .agg(F.collect_list(\"topic_id\").alias(\"topics\"))\n",
    "    .limit(10)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|document_id|topics_cnt|\n",
      "+-----------+----------+\n",
      "|     428514|         1|\n",
      "|    1951619|         1|\n",
      "|    1188934|         1|\n",
      "|    2259465|         1|\n",
      "|    2365393|         1|\n",
      "|    2450228|         1|\n",
      "|    2330926|         1|\n",
      "|     511261|         1|\n",
      "|    2432117|         1|\n",
      "|    2635049|         1|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "  documents_topics\n",
    "    .groupby('document_id')\n",
    "    .agg(F.count(\"topic_id\").alias(\"topics_cnt\"))\n",
    "    .orderBy(\"topics_cnt\")\n",
    "    .limit(10)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|document_id|topics_cnt|\n",
      "+-----------+----------+\n",
      "|    2733387|         1|\n",
      "|    2480990|         1|\n",
      "|    2604517|         1|\n",
      "|     769267|         1|\n",
      "|    2883834|         1|\n",
      "|    1591304|         1|\n",
      "|    2642433|         1|\n",
      "|    2152463|         1|\n",
      "|    2839332|         1|\n",
      "|    2961911|         1|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "  documents_topics\n",
    "    .groupby('document_id')\n",
    "    .agg(F.countDistinct(\"topic_id\").alias(\"topics_cnt\"))\n",
    "    .orderBy(\"topics_cnt\")\n",
    "    .limit(10)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|traffic_source|\n",
      "+--------------+\n",
      "|             3|\n",
      "|             1|\n",
      "|             2|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_views.select(\"traffic_source\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[traffic_source#338], functions=[])\n",
      "+- Exchange hashpartitioning(traffic_source#338, 200)\n",
      "   +- *(1) HashAggregate(keys=[traffic_source#338], functions=[])\n",
      "      +- *(1) Project [traffic_source#338]\n",
      "         +- *(1) FileScan parquet [traffic_source#338] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://ydatazian/page_views.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<traffic_source:string>\n"
     ]
    }
   ],
   "source": [
    "page_views.select(\"traffic_source\").distinct().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(document_id='1224367', topic_id='257', confidence_level='0.00800036766405164', row_number=0),\n",
       " Row(document_id='464561', topic_id='168', confidence_level='0.00800001705882854', row_number=0),\n",
       " Row(document_id='1556465', topic_id='6', confidence_level='0.00800008064000644', row_number=0),\n",
       " Row(document_id='489921', topic_id='31', confidence_level='0.0080008014972619', row_number=0),\n",
       " Row(document_id='1721729', topic_id='100', confidence_level='0.00800006482315529', row_number=0),\n",
       " Row(document_id='1648188', topic_id='268', confidence_level='0.00800010666666665', row_number=0),\n",
       " Row(document_id='1014485', topic_id='185', confidence_level='0.00800018794669673', row_number=0),\n",
       " Row(document_id='1382424', topic_id='99', confidence_level='0.00800032192007726', row_number=0),\n",
       " Row(document_id='1771673', topic_id='107', confidence_level='0.00800083967475122', row_number=0),\n",
       " Row(document_id='1210186', topic_id='160', confidence_level='0.00800055060004129', row_number=0),\n",
       " Row(document_id='1399822', topic_id='184', confidence_level='0.00800085901318781', row_number=0),\n",
       " Row(document_id='553205', topic_id='130', confidence_level='0.0080002070103306', row_number=0),\n",
       " Row(document_id='547836', topic_id='46', confidence_level='0.00800040584179321', row_number=0),\n",
       " Row(document_id='1571366', topic_id='219', confidence_level='0.00800020583123362', row_number=0),\n",
       " Row(document_id='1812731', topic_id='208', confidence_level='0.00800036676171826', row_number=0),\n",
       " Row(document_id='1543668', topic_id='171', confidence_level='0.00800380736393708', row_number=0),\n",
       " Row(document_id='935225', topic_id='71', confidence_level='0.00800009549206651', row_number=0),\n",
       " Row(document_id='1528667', topic_id='267', confidence_level='0.00800002816000525', row_number=0),\n",
       " Row(document_id='1713491', topic_id='110', confidence_level='0.00800004776978301', row_number=0),\n",
       " Row(document_id='2008655', topic_id='70', confidence_level='0.00800187440970282', row_number=0),\n",
       " Row(document_id='76361', topic_id='8', confidence_level='0.00800035650487743', row_number=0),\n",
       " Row(document_id='857730', topic_id='202', confidence_level='0.00800000023668639', row_number=0),\n",
       " Row(document_id='1618840', topic_id='22', confidence_level='0.00800032805621377', row_number=0),\n",
       " Row(document_id='803340', topic_id='206', confidence_level='0.00800026794670953', row_number=0),\n",
       " Row(document_id='1922221', topic_id='274', confidence_level='0.00800227633723358', row_number=0),\n",
       " Row(document_id='583662', topic_id='61', confidence_level='0.00800047574960686', row_number=0),\n",
       " Row(document_id='1784643', topic_id='218', confidence_level='0.00800120664882591', row_number=0),\n",
       " Row(document_id='2273558', topic_id='54', confidence_level='0.0080000612800098', row_number=0),\n",
       " Row(document_id='1369135', topic_id='276', confidence_level='0.00800056074546351', row_number=0),\n",
       " Row(document_id='1625533', topic_id='78', confidence_level='0.00800048447843313', row_number=0),\n",
       " Row(document_id='1744048', topic_id='98', confidence_level='0.00800159041703942', row_number=0),\n",
       " Row(document_id='1978450', topic_id='15', confidence_level='0.00800000856534265', row_number=0),\n",
       " Row(document_id='978451', topic_id='250', confidence_level='0.00800013986145446', row_number=0),\n",
       " Row(document_id='1545601', topic_id='188', confidence_level='0.00800438957516853', row_number=0),\n",
       " Row(document_id='654417', topic_id='284', confidence_level='0.00800013440001791', row_number=0),\n",
       " Row(document_id='1820377', topic_id='195', confidence_level='0.00800005376000286', row_number=0),\n",
       " Row(document_id='268936', topic_id='221', confidence_level='0.00800042476198341', row_number=0),\n",
       " Row(document_id='1173738', topic_id='273', confidence_level='0.00800010160002032', row_number=0),\n",
       " Row(document_id='325951', topic_id='112', confidence_level='0.00800007209638379', row_number=0),\n",
       " Row(document_id='839275', topic_id='163', confidence_level='0.00800007565433593', row_number=0),\n",
       " Row(document_id='2314403', topic_id='232', confidence_level='0.00800047837258846', row_number=0),\n",
       " Row(document_id='792988', topic_id='139', confidence_level='0.00800024587159722', row_number=0),\n",
       " Row(document_id='245374', topic_id='18', confidence_level='0.00800016426675426', row_number=0),\n",
       " Row(document_id='1772411', topic_id='224', confidence_level='0.00800066703451342', row_number=0),\n",
       " Row(document_id='1511542', topic_id='248', confidence_level='0.00800022223858919', row_number=0),\n",
       " Row(document_id='737192', topic_id='28', confidence_level='0.00800408302802019', row_number=0),\n",
       " Row(document_id='1500699', topic_id='203', confidence_level='0.00800034439649907', row_number=0),\n",
       " Row(document_id='1457228', topic_id='258', confidence_level='0.00800022348257593', row_number=0),\n",
       " Row(document_id='1373067', topic_id='120', confidence_level='0.00800022845571354', row_number=0),\n",
       " Row(document_id='2488273', topic_id='174', confidence_level='0.00800025676909156', row_number=0),\n",
       " Row(document_id='2281646', topic_id='75', confidence_level='0.00800059856285632', row_number=0),\n",
       " Row(document_id='870797', topic_id='234', confidence_level='0.00800000204081684', row_number=0),\n",
       " Row(document_id='170717', topic_id='73', confidence_level='0.008000059455261', row_number=0),\n",
       " Row(document_id='1126279', topic_id='27', confidence_level='0.00800065149060317', row_number=0),\n",
       " Row(document_id='1260717', topic_id='7', confidence_level='0.00800043733347276', row_number=0),\n",
       " Row(document_id='512601', topic_id='124', confidence_level='0.00800002659459315', row_number=0),\n",
       " Row(document_id='486896', topic_id='69', confidence_level='0.0080003358473275', row_number=0),\n",
       " Row(document_id='866764', topic_id='169', confidence_level='0.00800000064000004', row_number=0),\n",
       " Row(document_id='304916', topic_id='138', confidence_level='0.00800040000004986', row_number=0),\n",
       " Row(document_id='678058', topic_id='51', confidence_level='0.00800023923812941', row_number=0),\n",
       " Row(document_id='970998', topic_id='166', confidence_level='0.00800047377786989', row_number=0),\n",
       " Row(document_id='1737168', topic_id='132', confidence_level='0.00800024842107568', row_number=0),\n",
       " Row(document_id='1778182', topic_id='101', confidence_level='0.00800099213923395', row_number=0),\n",
       " Row(document_id='1568732', topic_id='272', confidence_level='0.00800282666666666', row_number=0),\n",
       " Row(document_id='356260', topic_id='162', confidence_level='0.00800095791321005', row_number=0),\n",
       " Row(document_id='1159159', topic_id='205', confidence_level='0.00800578470616592', row_number=0),\n",
       " Row(document_id='1378875', topic_id='34', confidence_level='0.00800002837333938', row_number=0),\n",
       " Row(document_id='620695', topic_id='113', confidence_level='0.00800034546120237', row_number=0),\n",
       " Row(document_id='287200', topic_id='155', confidence_level='0.00800009386417081', row_number=0),\n",
       " Row(document_id='25834', topic_id='29', confidence_level='0.00800011960079601', row_number=0),\n",
       " Row(document_id='1383148', topic_id='214', confidence_level='0.00800010052524643', row_number=0),\n",
       " Row(document_id='825697', topic_id='223', confidence_level='0.00800005536785343', row_number=0),\n",
       " Row(document_id='40574', topic_id='47', confidence_level='0.00800036176007957', row_number=0),\n",
       " Row(document_id='1053757', topic_id='42', confidence_level='0.00800010507773925', row_number=0),\n",
       " Row(document_id='194160', topic_id='200', confidence_level='0.00800008429378054', row_number=0),\n",
       " Row(document_id='1939630', topic_id='179', confidence_level='0.00800007203623664', row_number=0),\n",
       " Row(document_id='113635', topic_id='5', confidence_level='0.00800018262447156', row_number=0),\n",
       " Row(document_id='1476523', topic_id='59', confidence_level='0.00800086698171831', row_number=0),\n",
       " Row(document_id='911861', topic_id='3', confidence_level='0.008002472715809', row_number=0),\n",
       " Row(document_id='1450684', topic_id='187', confidence_level='0.00800008391306855', row_number=0),\n",
       " Row(document_id='297280', topic_id='287', confidence_level='0.00800007654822722', row_number=0),\n",
       " Row(document_id='788930', topic_id='296', confidence_level='0.00800029921495886', row_number=0),\n",
       " Row(document_id='1569466', topic_id='11', confidence_level='0.00800006131022799', row_number=0),\n",
       " Row(document_id='1193238', topic_id='279', confidence_level='0.008000047699535', row_number=0),\n",
       " Row(document_id='1176840', topic_id='146', confidence_level='0.00800000064000004', row_number=0),\n",
       " Row(document_id='1501149', topic_id='199', confidence_level='0.00800031722540601', row_number=0),\n",
       " Row(document_id='705829', topic_id='125', confidence_level='0.00800004807512413', row_number=0),\n",
       " Row(document_id='378282', topic_id='64', confidence_level='0.00800109043286055', row_number=0),\n",
       " Row(document_id='2705772', topic_id='43', confidence_level='0.00800011628122133', row_number=0),\n",
       " Row(document_id='107970', topic_id='87', confidence_level='0.00800030024941398', row_number=0),\n",
       " Row(document_id='97253', topic_id='96', confidence_level='0.00800002709333477', row_number=0),\n",
       " Row(document_id='2810973', topic_id='133', confidence_level='0.00800204134697929', row_number=0),\n",
       " Row(document_id='147341', topic_id='183', confidence_level='0.00800056864897607', row_number=0),\n",
       " Row(document_id='735672', topic_id='0', confidence_level='0.00800023470954348', row_number=0),\n",
       " Row(document_id='1390144', topic_id='85', confidence_level='0.00800047605123181', row_number=0),\n",
       " Row(document_id='1573296', topic_id='30', confidence_level='0.00800008651687822', row_number=0),\n",
       " Row(document_id='1535749', topic_id='154', confidence_level='0.00800017773302881', row_number=0),\n",
       " Row(document_id='220931', topic_id='282', confidence_level='0.00800004623025456', row_number=0),\n",
       " Row(document_id='906719', topic_id='298', confidence_level='0.00800137745468856', row_number=0),\n",
       " Row(document_id='763085', topic_id='118', confidence_level='0.00800216577540107', row_number=0)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Window.partitionBy('topic_id')\\\n",
    "          .orderBy('confidence_level')\n",
    "(\n",
    "  documents_topics\n",
    "    .withColumn('row_number', F.row_number().over(w) - 1)\n",
    "    .orderBy(\"row_number\")\n",
    "    .take(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hK93Ci6Rj51c"
   },
   "source": [
    "## HW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Jf8VBI-j530"
   },
   "source": [
    "Dataset: outbrain click prediction\n",
    "\n",
    "Tasks:\n",
    "using Spark RDD, DataFrame API and Python, complete following tasks:\n",
    "\n",
    "**1 (1 point)**. Find 10 most visited document_ids in page_views log\n",
    "\n",
    "**2 (1 point)**. Find out how many users have at least two different traffic_sources in their page_views log\n",
    "\n",
    "**3* (1 additional point)**. Find 10 most visited topic_ids in page_views log (use documents_topics table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqXylvo5kJa-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_seminar (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
